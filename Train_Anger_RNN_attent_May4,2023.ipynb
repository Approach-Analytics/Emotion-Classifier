{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Approach-Analytics/Emotion-Classifier/blob/main/Train_Anger_RNN_attent_May4%2C2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aNbKs-0wBj0"
      },
      "source": [
        "#Anger Training run - Last updated May 5,2023\n",
        "\n",
        "Synchronize this notebook with the new dataset creation capacity. We also want to try to synchronize the word distribution as well... \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf-ueAD8v7Td",
        "outputId": "9b873e1a-33bf-4197-f00a-d5f23348ec83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nlp) (1.22.4)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from nlp) (3.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nlp) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from nlp) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from nlp) (4.65.0)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->nlp) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, nlp\n",
            "Successfully installed dill-0.3.6 nlp-0.4.0 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "#Need pip install tensorflow with...Jupyter Notebook\n",
        "\n",
        "!pip install nlp\n",
        "#!pip install tensorflow\n",
        "!pip install h5py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1JrYFgcnwaaJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F49j2PXxwdjv"
      },
      "outputs": [],
      "source": [
        "#Dec 7, 2022: Not sure what this piece of code is used for... \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "50a7uMJE_1zJ"
      },
      "outputs": [],
      "source": [
        "#Setting column options\n",
        "\n",
        "pd.set_option('display.width', 200)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "#Supressing the scientific notation \n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import the Tokenizer"
      ],
      "metadata": {
        "id": "11NrSp4UPXJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_4oC8eawtYt",
        "outputId": "9dbd6ab2-b182-4860-9517-98bb65f7c13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the Gutenberg universal tokenizer... \n",
        "\n",
        "path = '/content/drive/MyDrive/Sean/Emoclass/Tokenizer_Universal_125Kvoc_May5.2023.pickle'\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "by-HjrToPaN-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing inference dataset"
      ],
      "metadata": {
        "id": "YRiiuEiOqZjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load inference dataset...Let's use df1 so there are no mix ups \n",
        "\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/Sean/Twitter/COVID_223KTweets.csv\")"
      ],
      "metadata": {
        "id": "erHCPRuIqZSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "3bgPTD90qyuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create word length function... \n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())"
      ],
      "metadata": {
        "id": "ZSCjy-gAqZOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the function... \n",
        "\n",
        "df1['word_count'] = df1['tweet'].apply(count_words)"
      ],
      "metadata": {
        "id": "wxqNjKmzqZJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHF09wSweqz"
      },
      "source": [
        "##Importing dataset and defining emotion axis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rAr8P5A4kZA"
      },
      "outputs": [],
      "source": [
        "#Anger Schema - May 4,2023  \n",
        "#If we call it emo_axis -> we don't have to change it across capabilities....\n",
        "\n",
        "emo_axis = {\n",
        "    \"rage\": ['rage', 'raging', 'raged', 'fury', 'wrath', 'wrathful', 'furious', 'enraged', 'enraging', 'incensed', 'outraged'],\n",
        "    \"angry\": [\"angry\", \"anger\", \"angered\", \"mad\", \"maddening\", \"maddened\"],\n",
        "    \"frustrated\": [\"frustration\", \"frustrate\", \"frustrated\", \"frustrating\", \"exasperated\", \"exasperating\", \"discontented\", \"vexed\", \"vexing\", \"bothered\", \"bothersome\"],\n",
        "    \"agitated\": [\"agitated\", \"agitating\", \"agitation\", \"aggravated\", \"aggravation\", \"aggravating\", \"upset\", 'upsetting', 'irritate', 'irritating', 'irritable', 'irritated'],\n",
        "    \"annoyed\": [\"annoying\", \"annoy\", \"annoyed\", \"pestered\", \"pestering\", \"pester\", \"troubled\", \"troubling\", \"disturbed\", \"disturbing\", \"harassed\", \"harassing\", \"nagged\", \"nagging\"],\n",
        "    \"calm\": [\"calm\", \"peaceful\", \"serene\", \"serenity\", \"relax\", \"relaxing\", \"relaxed\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nURRLtOYwv67"
      },
      "outputs": [],
      "source": [
        "#Load the dataset \n",
        "\n",
        "path = \"/content/drive/MyDrive/Sean/Emoclass/Emotion datasets/Anger_G2000_1million_May4,2023.csv\"\n",
        "df=pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7ho0hEJ6VVa"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXmCKFFZ7DFl"
      },
      "outputs": [],
      "source": [
        "#Looking at the dataset \n",
        "\n",
        "#df.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAA_wiJS7L5M"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWVVU1YDw4aU"
      },
      "outputs": [],
      "source": [
        "#Relabelling a column... if needed... \n",
        "\n",
        "df.rename(columns={\"emotion2\": \"label\"},inplace =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_4MMzZ1zguA"
      },
      "outputs": [],
      "source": [
        "#Checking to see if classes are balanced or not...\n",
        "\n",
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpxXO76jzEPE"
      },
      "source": [
        "#Word length distribution... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh8bGSpyqDTT"
      },
      "outputs": [],
      "source": [
        "#Apply function to calculate word length\n",
        "\n",
        "df['word_count'] = df['text'].apply(count_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENpPmMDCqJCd"
      },
      "outputs": [],
      "source": [
        "df['word_count'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shuffling and Truncating Distribution "
      ],
      "metadata": {
        "id": "VkhPxv48tQ-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the DataFrame\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Reset the index of the shuffled DataFrame\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "MyvczHJUWPMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a quantile list\n",
        "quantile_list = [i / 100 for i in range(41)]\n",
        "\n",
        "print(quantile_list)\n",
        "\n",
        "# Calculate quantiles for both datasets using the quantile list\n",
        "quantiles_df = pd.DataFrame({\n",
        "    'Inference_dataset': df1['word_count'].quantile(quantile_list),\n",
        "    'Training_dataset': df['word_count'].quantile(quantile_list)\n",
        "})\n",
        "\n",
        "# Calculate the difference between quantile values for each dataset\n",
        "quantiles_df['Difference'] = quantiles_df['Training_dataset'] - quantiles_df['Inference_dataset']\n",
        "\n",
        "# Define a function to get the quantile difference for a given word_count value\n",
        "def get_quantile_diff(word_count, quantiles_df):\n",
        "    for i in range(len(quantile_list) - 1):\n",
        "        if word_count <= quantiles_df.loc[quantile_list[i+1], 'Training_dataset']:\n",
        "            return quantiles_df.loc[quantile_list[i], 'Difference']\n",
        "    return quantiles_df.loc[quantile_list[-1], 'Difference']\n",
        "\n",
        "# Calculate 'quantile_diff' for each row in the 'df' DataFrame\n",
        "df['quantile_diff'] = df['word_count'].apply(lambda x: get_quantile_diff(x, quantiles_df))\n",
        "\n",
        "# Display the resulting dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "tLK84yhu6185"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function to remove text from both ends based on difference in values in quantiles...\n",
        "\n",
        "def truncate_text(text, quantile_diff):\n",
        "    words = text.split()\n",
        "    words_to_remove = int(abs(quantile_diff))\n",
        "    \n",
        "    if words_to_remove >= len(words):\n",
        "        return \"\"\n",
        "    \n",
        "    if quantile_diff > 0:\n",
        "        # Remove words equally from the beginning and the end\n",
        "        start_index = words_to_remove // 2\n",
        "        end_index = -(words_to_remove - start_index)\n",
        "        if end_index == 0:\n",
        "            truncated_words = words[start_index:]\n",
        "        else:\n",
        "            truncated_words = words[start_index:end_index]\n",
        "    else:\n",
        "        # Add placeholder words to the beginning and the end\n",
        "        placeholder = \"<placeholder>\"\n",
        "        padding = [placeholder] * (words_to_remove // 2)\n",
        "        truncated_words = padding + words + padding\n",
        "    \n",
        "    truncated_text = ' '.join(truncated_words)\n",
        "    return truncated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "_Dm_3HVUOiJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncate the text in the 'text' column based on the 'quantile_diff' values\n",
        "df['truncated_text'] = df.apply(lambda row: truncate_text(row['text'], row['quantile_diff']), axis=1)\n",
        "\n",
        "# Display the resulting dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "j5Zp-gQ5OiIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reapply word count\n",
        "\n",
        "df['trunc_word_counts'] = df['truncated_text'].apply(count_words)"
      ],
      "metadata": {
        "id": "HO2TJNoR8Q0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['trunc_word_counts'].describe()"
      ],
      "metadata": {
        "id": "Gs0eXQ-I9CpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(df['trunc_word_counts'], bins=200)\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Truncated Word Count Histogram')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XkQAjWNO8QxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Reassigning Truncated Text Variable Back "
      ],
      "metadata": {
        "id": "H5hmtMVYQnjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['truncated_text']"
      ],
      "metadata": {
        "id": "Z4h7EVnn8Qtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ibTfjthxBav"
      },
      "source": [
        "##Replacing the emotion words in the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_g8yc8uxFT6"
      },
      "outputs": [],
      "source": [
        "#Generate all the unique emotion words that then get replaced... \n",
        "\n",
        "a = df['emotion1'].unique().tolist()\n",
        "\n",
        "#The list a is our list of variable responses from the dataset... \n",
        "keyword = \"emotions\"\n",
        "words = a\n",
        "for j in words: \n",
        "  df['text'] = df['text'].str.replace(j,keyword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9BLAlNDVSo"
      },
      "source": [
        "##Counting vocab size "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define a function to split text into words\n",
        "def split_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Split the text in the 'text' column and count the occurrences of each word\n",
        "word_counter = Counter()\n",
        "for text in df['text']:\n",
        "    words = split_text(text)\n",
        "    word_counter.update(words)\n",
        "\n",
        "# Calculate the vocabulary size\n",
        "vocab_size = len(word_counter)\n",
        "\n",
        "print(f\"The vocabulary size is: {vocab_size}\")\n"
      ],
      "metadata": {
        "id": "H5TmuwuNQKjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyEbG1u2zkZW"
      },
      "source": [
        "##Splitting into train, validate and test datasets inluding shuffle dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPfMTt6eznQx"
      },
      "outputs": [],
      "source": [
        "#This outputs 3 different dataframes... originally 0.6 and 0.8\n",
        "\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=42),\n",
        "                       [int(.8*len(df)), int(.9*len(df))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LThCDRYUzp5O"
      },
      "source": [
        "##Preparing Labels - Need to change emotion axis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYKZBDB3zs_n"
      },
      "outputs": [],
      "source": [
        "#Converting the pandas dataframe into a list of labels... \n",
        "#We may consider puting this into a function... \n",
        "\n",
        "trainlabel=train['label'].tolist()\n",
        "vallabel=validate['label'].tolist()\n",
        "testlabel=test['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yqHv7PJ4bBe"
      },
      "outputs": [],
      "source": [
        "#Creating the classes variable... with new emotion classifier..\n",
        "# loneliness_axis\n",
        "\n",
        "classes = list(emo_axis.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMs8eE1QzvLT"
      },
      "outputs": [],
      "source": [
        "# Map each class to a unique integer\n",
        "classes_to_index = dict((c, i) for i, c in enumerate(classes))\n",
        "\n",
        "# Map each integer back to its corresponding class\n",
        "index_to_classes = dict((v, k) for k, v in classes_to_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JcN3l8o0EKH"
      },
      "outputs": [],
      "source": [
        "#Creating a lambda function...called \"names_to_ids\"\n",
        "\n",
        "names_to_ids = lambda labels: np.array([classes_to_index.get(x) for x in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t86jjfI0FkB"
      },
      "outputs": [],
      "source": [
        "#Applying the names_to_ids functions to the labels\n",
        "\n",
        "train_labels = names_to_ids(trainlabel)\n",
        "val_labels = names_to_ids(vallabel)\n",
        "test_labels = names_to_ids(testlabel)\n",
        "\n",
        "#Testing out the labels...\n",
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-2CmSxR3Std"
      },
      "outputs": [],
      "source": [
        "#What is train_labels?\n",
        "\n",
        "train_labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIvdXKMr3wLS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use numpy's unique() and return_counts=True parameters to get value counts\n",
        "values, counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "# Create a dictionary with the values and their respective counts\n",
        "value_counts = dict(zip(values, counts))\n",
        "\n",
        "# Print the dictionary\n",
        "print(value_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCIoamZJ0HU7"
      },
      "source": [
        "##Input Training Text and Tokenizing Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujG0RIpm0MAn"
      },
      "outputs": [],
      "source": [
        "#Converting the different pandas dataframes into a list of text fields...\n",
        "#Choice of variables: truncated_text, text_minus1, 'filtered_text',''filtered_text_minus1'\n",
        "\n",
        "traintext=train['text'].tolist()\n",
        "valtext=validate['text'].tolist()\n",
        "testtext=test['text'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trying \"universal\" tokenizer - trained on Gutenberg"
      ],
      "metadata": {
        "id": "gztj1JC-IHif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1ngFMxo0RIZ"
      },
      "outputs": [],
      "source": [
        "#Importing the tokenizer...\n",
        "#Input into the tokenizer is a list\n",
        "\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vZwl0WF0TQ6"
      },
      "outputs": [],
      "source": [
        "#Input into the tokenizer is a list\n",
        "\n",
        "#tokenizer = Tokenizer(num_words=131000, oov_token='<UNK>')\n",
        "\n",
        "#I think that this is the missing piece...I'm not really sure what it does...  \n",
        "\n",
        "#tokenizer.fit_on_texts(traintext)\n",
        "#tokenizer.fit_on_texts(valtext)\n",
        "\n",
        "#Testing the tokenization... \n",
        "\n",
        "#print(tokenizer.texts_to_sequences([tweets[10]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8HPMv5G0VVO"
      },
      "source": [
        "#Tokenizing and padding \n",
        "\n",
        "##Pay attention to where tokenizer coming from..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb6j0LVO0XN0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tx-hkxo0aXP"
      },
      "outputs": [],
      "source": [
        "#Creating a function that tokenizes and pads the sequences...\n",
        "#Max word length is 65...\n",
        "\n",
        "def get_sequences(tokenizer, texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=65, padding='post')\n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUm5oBbQ0chu"
      },
      "outputs": [],
      "source": [
        "#Applying the function to tokenize and pad... to all test,validate and test\n",
        "#Syntax: val_sequences = get_sequences(tokenizer, val_tweets)\n",
        "\n",
        "padded_train_sequences = get_sequences(tokenizer, traintext)\n",
        "val_sequence = get_sequences(tokenizer,valtext)\n",
        "test_sequence = get_sequences(tokenizer,testtext)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1WyE_0_8r-1"
      },
      "source": [
        "#Adding attention layer and specifying the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxWvy7498eWt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return tf.keras.backend.sum(output, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcMIM3IFY57T"
      },
      "outputs": [],
      "source": [
        "#Specifying the model - words: 76690\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(76990, 200, input_length=65),\n",
        "    tf.keras.layers.Conv1D(200, 5, activation='relu'),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, return_sequences=True)),\n",
        "    Attention(),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WftFq18v-IIR"
      },
      "source": [
        "##Running the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7Gu2xnW0nGa"
      },
      "outputs": [],
      "source": [
        "#Most up to date architecture: April 15, 2023 -> Fa\n",
        "\n",
        "#Having the callbacks option on means that the model stops once you are like over-generalizing... \n",
        "#Let's leave the callback option on for now...\n",
        "\n",
        "\n",
        "h = model.fit(\n",
        "    padded_train_sequences, train_labels,\n",
        "    validation_data=(val_sequence, val_labels),\n",
        "    epochs=5,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9vH4glT0pvr"
      },
      "source": [
        "#Evaluating The Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D4Kl54K0rww"
      },
      "outputs": [],
      "source": [
        "def show_history(h):\n",
        "    epochs_trained = len(h.history['loss'])\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n",
        "    plt.ylim([0., 1.])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "show_history(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKJGeWYD0uKW"
      },
      "outputs": [],
      "source": [
        "#Running the model on the test sequence and test labels... \n",
        "\n",
        "eval = model.evaluate(test_sequence, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVtCPVVV0wMU"
      },
      "outputs": [],
      "source": [
        "#preds = model.predict_classes(test_sequence)\n",
        "preds=model.predict(test_sequence) \n",
        "classes_x=np.argmax(preds,axis=1)\n",
        "preds.shape, test_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi85BmDv0xwm"
      },
      "source": [
        "##Error Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0bPxr94oPwj"
      },
      "outputs": [],
      "source": [
        "#Dumb luck metric \n",
        "\n",
        "counts = df['label'].value_counts()\n",
        "dumb_luck = max(counts) / sum(counts)\n",
        "\n",
        "dumb_luck\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtVkH2y70zda"
      },
      "outputs": [],
      "source": [
        "#Creating an inference dataframe \n",
        "\n",
        "inf_df=pd.DataFrame({\n",
        "    'data':testtext,\n",
        "    \"labels_predicted\": classes_x                    \n",
        "})\n",
        "inf_df[\"labels_predicted_marked\"]=inf_df['labels_predicted'].apply(lambda x: index_to_classes[x])\n",
        "inf_df[\"actual_labels\"]=testlabel\n",
        "\n",
        "#Creating the labels index datastructure...\n",
        "\n",
        "inf_df[\"actual_label_index\"]=inf_df['actual_labels'].apply(lambda x: classes_to_index[x])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frs6-UaJ05J0"
      },
      "outputs": [],
      "source": [
        "#Checking the inf_df shape...\n",
        "\n",
        "inf_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djekeBFK06Xu"
      },
      "outputs": [],
      "source": [
        "#Making the correct prediction \n",
        "\n",
        "inf_df['correct_pred']=0 # first assigning all to 0.\n",
        "inf_df.loc[(inf_df['labels_predicted']==inf_df['actual_label_index']),'correct_pred']=1 # labelling 1 if the prediction is right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc0hQQnc0829"
      },
      "outputs": [],
      "source": [
        "# magnitutde of error\n",
        "inf_df['error_magnitude']=abs(inf_df['labels_predicted']-inf_df['actual_label_index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKRJBQcf0-X5"
      },
      "outputs": [],
      "source": [
        "# Count the frequency of each class in the error_magnitude column\n",
        "counts = inf_df['error_magnitude'].value_counts()\n",
        "\n",
        "# Create a histogram with one bar for each class\n",
        "plt.bar(counts.index, counts.values)\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.suptitle('Histogram of Error Magnitude in Fear Classification')\n",
        "plt.xlabel('Magnitude')\n",
        "plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWaZuOGY1CFg"
      },
      "source": [
        "##Saving the model, tokenizer and evaluation datasets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMWl2ote91Bx"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Make sure you provide the full file path, including the .h5 extension\n",
        "\n",
        "model.save('/content/drive/MyDrive/Sean/Emoclass/Model_Anger_1mil_dist_May5,2023.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTNJKbfw1NDi"
      },
      "outputs": [],
      "source": [
        "#Exporting the dataset \n",
        "\n",
        "#path = \"/content/drive/MyDrive/Sean/Emoclass/Train_Fear_Error_Analysis_151K_March8,2023.csv\"\n",
        "inf_df.to_csv(path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNc6IVSqZa3njNWxJ1XlpL0",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}