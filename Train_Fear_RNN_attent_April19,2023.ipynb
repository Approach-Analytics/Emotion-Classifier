{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Approach-Analytics/Emotion-Classifier/blob/main/Train_Fear_RNN_attent_April19%2C2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aNbKs-0wBj0"
      },
      "source": [
        "#Despair: Refactored training run - April 12,2023\n",
        "\n",
        "Synchronize this notebook with the new dataset creation capacity. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf-ueAD8v7Td",
        "outputId": "3a845df6-29e3-4a92-bcfe-7070d4998384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp\n",
            "  Downloading nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from nlp) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from nlp) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from nlp) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from nlp) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.9/dist-packages (from nlp) (9.0.0)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from nlp) (3.11.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->nlp) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->nlp) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, nlp\n",
            "Successfully installed dill-0.3.6 nlp-0.4.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "#Need pip install tensorflow with...Jupyter Notebook\n",
        "\n",
        "!pip install nlp\n",
        "#!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1JrYFgcnwaaJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F49j2PXxwdjv"
      },
      "outputs": [],
      "source": [
        "#Dec 7, 2022: Not sure what this piece of code is used for... \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "50a7uMJE_1zJ"
      },
      "outputs": [],
      "source": [
        "#Setting column options\n",
        "\n",
        "pd.set_option('display.width', 200)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "#Supressing the scientific notation \n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHF09wSweqz"
      },
      "source": [
        "##Importing dataset and fear_axis definition "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0rAr8P5A4kZA"
      },
      "outputs": [],
      "source": [
        "#Defining the despair axis\n",
        "#Removed: April 12, 2023: \"content\"\n",
        "\n",
        "#Defining the fear axis \n",
        "\n",
        "fear_axis= {\n",
        "    \"dread\" : ['panic','dread','horror','horrified','horrifying','terror','terrified','terrifying'],\n",
        "    \"fear\":  [\"fear\", \"fearful\", \"fright\",\"frightening\",\"afraid\",\"frightful\",\"frightfully\",\"frightened\",'scared','scary','scare'],\n",
        "    \"anxiety\": [\"anxious\",\"anxiety\",'angst','anxiousness'],\n",
        "    \"worry\":[\"worry\", \"worried\",\"worrying\",\"worries\"],\n",
        "    \"concern\": [\"concern\", \"concerning\",\"concerned\",\"concerns\"],\n",
        "    \"calm\": [\"calm\",\"peaceful\",\"serene\",\"serenity\",\"untroubled\", \"content\", \"contented\", \"composed\", \"tranquil\"]\n",
        "    }\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_4oC8eawtYt",
        "outputId": "9c59b1d7-1131-4492-b282-25cfbee7c75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nURRLtOYwv67"
      },
      "outputs": [],
      "source": [
        "#Load the dataset \n",
        "\n",
        "\n",
        "#path = \"/content/drive/MyDrive/Sean/Emoclass/Emotion datasets/Fear_G2000_266K_April14,2023.csv\"\n",
        "\n",
        "\n",
        "path = \"/content/drive/MyDrive/Sean/Emoclass/Emotion datasets/Fear_G2000_1.4mil_April19,2023.csv\"\n",
        "df=pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7ho0hEJ6VVa",
        "outputId": "9b48665d-c7f4-4543-9031-24699e5a7c67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1428572, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5550
        },
        "id": "LXmCKFFZ7DFl",
        "outputId": "823b1f50-c210-450b-a15a-50a16b28d9e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text   emotion1    label  word_length treatment  new_word_length  word_count\n",
              "0   seeming to wait for me and now everything in t...       calm     calm           77   Nothing               77          77\n",
              "1   would never behave as he does to another guard...      worry    worry           80   Nothing               80          80\n",
              "2   section there is also the seamen pay branch wh...    concern  concern           68   Nothing               68          68\n",
              "3   seemed to be well oriented and conscious of ev...  contented     calm           68   Nothing               68          68\n",
              "4   two sides he stuffed cold bannock into the poc...      panic    dread           72   Nothing               72          72\n",
              "..                                                ...        ...      ...          ...       ...              ...         ...\n",
              "95  by the time we arrived in oakland i was as lim...    anxious  anxiety           76   Nothing               76          76\n",
              "96  details some strong words shall express what i...     fright     fear           69   Nothing               69          69\n",
              "97  which they make to furnish their huge carcasse...     fright     fear           70   Nothing               70          70\n",
              "98  to whose grief must be added without any fault...      worry    worry           84   Nothing               84          84\n",
              "99  the question before her in less than five minu...     afraid     fear           76   Nothing               76          76\n",
              "\n",
              "[100 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-740001d4-23ed-4f27-917c-57ab456f5ffd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>emotion1</th>\n",
              "      <th>label</th>\n",
              "      <th>word_length</th>\n",
              "      <th>treatment</th>\n",
              "      <th>new_word_length</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>seeming to wait for me and now everything in t...</td>\n",
              "      <td>calm</td>\n",
              "      <td>calm</td>\n",
              "      <td>77</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>77</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>would never behave as he does to another guard...</td>\n",
              "      <td>worry</td>\n",
              "      <td>worry</td>\n",
              "      <td>80</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>section there is also the seamen pay branch wh...</td>\n",
              "      <td>concern</td>\n",
              "      <td>concern</td>\n",
              "      <td>68</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>seemed to be well oriented and conscious of ev...</td>\n",
              "      <td>contented</td>\n",
              "      <td>calm</td>\n",
              "      <td>68</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>68</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>two sides he stuffed cold bannock into the poc...</td>\n",
              "      <td>panic</td>\n",
              "      <td>dread</td>\n",
              "      <td>72</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>72</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>by the time we arrived in oakland i was as lim...</td>\n",
              "      <td>anxious</td>\n",
              "      <td>anxiety</td>\n",
              "      <td>76</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>76</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>details some strong words shall express what i...</td>\n",
              "      <td>fright</td>\n",
              "      <td>fear</td>\n",
              "      <td>69</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>69</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>which they make to furnish their huge carcasse...</td>\n",
              "      <td>fright</td>\n",
              "      <td>fear</td>\n",
              "      <td>70</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>70</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>to whose grief must be added without any fault...</td>\n",
              "      <td>worry</td>\n",
              "      <td>worry</td>\n",
              "      <td>84</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>84</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>the question before her in less than five minu...</td>\n",
              "      <td>afraid</td>\n",
              "      <td>fear</td>\n",
              "      <td>76</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>76</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-740001d4-23ed-4f27-917c-57ab456f5ffd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-740001d4-23ed-4f27-917c-57ab456f5ffd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-740001d4-23ed-4f27-917c-57ab456f5ffd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "df.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAA_wiJS7L5M",
        "outputId": "4cd78487-e9fd-452b-ba4c-8d1a746b9fc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'emotion1', 'emotion2', 'word_length', 'treatment', 'new_word_length'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YWVVU1YDw4aU"
      },
      "outputs": [],
      "source": [
        "#Relabelling a column... if needed... \n",
        "\n",
        "df.rename(columns={\"emotion2\": \"label\"},inplace =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz0wm7a_w46o",
        "outputId": "7fe25b13-1dfd-4fa7-9e75-30cf3885a5cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text               0\n",
              "emotion1           0\n",
              "label              0\n",
              "word_length        0\n",
              "treatment          0\n",
              "new_word_length    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Let's see if we have missing values...No missing values... \n",
        "\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpxXO76jzEPE"
      },
      "source": [
        "#Truncating Word Length "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Eh8bGSpyqDTT"
      },
      "outputs": [],
      "source": [
        "#Defining function to calculate word length\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "df['word_count'] = df['text'].apply(count_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENpPmMDCqJCd",
        "outputId": "f2676f8a-e87e-453c-c824-51867109b2d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count   1428572.000\n",
              "mean         49.925\n",
              "std          21.537\n",
              "min           5.000\n",
              "25%          33.000\n",
              "50%          43.000\n",
              "75%          72.000\n",
              "max          99.000\n",
              "Name: word_count, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df['word_count'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FBISwaQKzH9D"
      },
      "outputs": [],
      "source": [
        "def truncate_text(text, max_len, keyword):\n",
        "    if len(text) <= max_len:\n",
        "        return text\n",
        "    \n",
        "    # Find the index of the keyword in the text\n",
        "    keyword_idx = text.find(keyword)\n",
        "    \n",
        "    # If the keyword is not found or is too close to the ends of the text, just truncate from the ends\n",
        "    if keyword_idx == -1 or keyword_idx < max_len//2 or keyword_idx > len(text) - max_len//2:\n",
        "        return text[:max_len]\n",
        "    \n",
        "    # Truncate from both sides\n",
        "    start_idx = keyword_idx - max_len//2\n",
        "    end_idx = keyword_idx + max_len//2\n",
        "    return text[start_idx:end_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nf8p5Vkm-2_8"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the 'text' column and create a new column 'truncated_text'\n",
        "#df['truncated_text'] = df.apply(lambda row: truncate_text(row['text'], max_len=100, keyword='emotion1'), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yHzy_cSZBq8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "af3e3155-8de2-401c-e760-62cdc168d742"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'truncated_text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4f8f111ec067>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Redoing the word count....I think this needs to get reapplied...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_count_trunc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truncated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'truncated_text'"
          ]
        }
      ],
      "source": [
        "#Redoing the word count....I think this needs to get reapplied... \n",
        "\n",
        "#df[\"word_count_trunc\"] = df['truncated_text'].apply(count_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSKkAJsp_InD"
      },
      "outputs": [],
      "source": [
        "#Checking word length again... \n",
        "\n",
        "#df['word_count_trunc'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ibTfjthxBav"
      },
      "source": [
        "##Replacing the emotion words in the dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "L_g8yc8uxFT6"
      },
      "outputs": [],
      "source": [
        "#Generate all the unique emotion words that then get replaced... \n",
        "\n",
        "a = df['emotion1'].unique().tolist()\n",
        "\n",
        "#The list a is our list of variable responses from the dataset... \n",
        "keyword = \"emotion word\"\n",
        "words = a\n",
        "for j in words: \n",
        "  df['text'] = df['text'].str.replace(j,keyword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9BLAlNDVSo"
      },
      "source": [
        "##Counting vocab size "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BaGR5rfwDX4F"
      },
      "outputs": [],
      "source": [
        "#Total vocabulary size prior to truncation \n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# convert the text into a list\n",
        "text = df['text'].tolist()\n",
        "\n",
        "# create a Tokenizer object\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit the tokenizer on the text data\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "# calculate the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36WeYWzXDZU2",
        "outputId": "a7ab87fb-3489-47fe-d9c1-e309804ae63f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127780"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyEbG1u2zkZW"
      },
      "source": [
        "##Splitting into train, validate and test datasets inluding shuffle dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iPfMTt6eznQx"
      },
      "outputs": [],
      "source": [
        "#This outputs 3 different dataframes... originally 0.6 and 0.8\n",
        "\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=42),\n",
        "                       [int(.8*len(df)), int(.9*len(df))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LThCDRYUzp5O"
      },
      "source": [
        "##Preparing Labels - Needs manual label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tYKZBDB3zs_n"
      },
      "outputs": [],
      "source": [
        "#Converting the pandas dataframe into a list of labels... \n",
        "#We may consider puting this into a function... \n",
        "\n",
        "trainlabel=train['label'].tolist()\n",
        "vallabel=validate['label'].tolist()\n",
        "testlabel=test['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6yqHv7PJ4bBe"
      },
      "outputs": [],
      "source": [
        "#Creating the classes variable... \n",
        "\n",
        "classes = list(fear_axis.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IMs8eE1QzvLT"
      },
      "outputs": [],
      "source": [
        "# Map each class to a unique integer\n",
        "classes_to_index = dict((c, i) for i, c in enumerate(classes))\n",
        "\n",
        "# Map each integer back to its corresponding class\n",
        "index_to_classes = dict((v, k) for k, v in classes_to_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1JcN3l8o0EKH"
      },
      "outputs": [],
      "source": [
        "#Creating a lambda function...called \"names_to_ids\"\n",
        "\n",
        "names_to_ids = lambda labels: np.array([classes_to_index.get(x) for x in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t86jjfI0FkB",
        "outputId": "319def9f-13a3-4dbd-e329-bfb97801f3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "#Applying the names_to_ids functions to the labels\n",
        "\n",
        "train_labels = names_to_ids(trainlabel)\n",
        "val_labels = names_to_ids(vallabel)\n",
        "test_labels = names_to_ids(testlabel)\n",
        "\n",
        "#Testing out the labels...\n",
        "print(train_labels[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCIoamZJ0HU7"
      },
      "source": [
        "##Input Training Text and Tokenizing Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ujG0RIpm0MAn"
      },
      "outputs": [],
      "source": [
        "#Converting the different pandas dataframes into a list of text fields...\n",
        "#Choice of variables: truncated_text, text_minus1, 'filtered_text',''filtered_text_minus1'\n",
        "\n",
        "traintext=train['text'].tolist()\n",
        "valtext=validate['text'].tolist()\n",
        "testtext=test['text'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "r1ngFMxo0RIZ"
      },
      "outputs": [],
      "source": [
        "#Importing the tokenizer...\n",
        "#Input into the tokenizer is a list\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2vZwl0WF0TQ6"
      },
      "outputs": [],
      "source": [
        "#Input into the tokenizer is a list\n",
        "\n",
        "tokenizer = Tokenizer(num_words=128945, oov_token='<UNK>')\n",
        "\n",
        "#I think that this is the missing piece...I'm not really sure what it does...  \n",
        "\n",
        "tokenizer.fit_on_texts(traintext)\n",
        "#tokenizer.fit_on_texts(valtext)\n",
        "\n",
        "#Testing the tokenization... \n",
        "\n",
        "#print(tokenizer.texts_to_sequences([tweets[10]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8HPMv5G0VVO"
      },
      "source": [
        "#Padding and Truncating Sequences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yb6j0LVO0XN0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0tx-hkxo0aXP"
      },
      "outputs": [],
      "source": [
        "#Creating a function that tokenizes and pads the sequences...\n",
        "\n",
        "def get_sequences(tokenizer, texts):\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=300, padding='post')\n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZUm5oBbQ0chu"
      },
      "outputs": [],
      "source": [
        "#Applying the function to tokenize and pad... to all test,validate and test\n",
        "#Syntax: val_sequences = get_sequences(tokenizer, val_tweets)\n",
        "\n",
        "padded_train_sequences = get_sequences(tokenizer, traintext)\n",
        "val_sequence = get_sequences(tokenizer,valtext)\n",
        "test_sequence = get_sequences(tokenizer,testtext)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl6l2FtS0ecg"
      },
      "source": [
        "##Creating and training the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "FfqZq2Hw0iGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73fb30c-3e5c-407b-85c2-edf021886155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 400)          51578000  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 300, 800)         2563200   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 800)              3843200   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 4806      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,989,206\n",
            "Trainable params: 57,989,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Embedding layer works on number of tokens -> approximately words... \n",
        "#Adding another layer: tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(500, return_sequences=True)),\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(128945, 400, input_length=300),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(400, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(400)),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1WyE_0_8r-1"
      },
      "source": [
        "#Adding an attention layer to the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pxWvy7498eWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2244f7b1-3d63-40d6-c142-d1d23eb8a71a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 200, 400)          51578000  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 200, 400)         961600    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " attention (Attention)       (None, 400)               600       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 2406      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,542,606\n",
            "Trainable params: 52,542,606\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
        "        a = tf.keras.backend.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return tf.keras.backend.sum(output, axis=1)\n",
        "\n",
        "# Define your model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(128945, 400, input_length=200),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200, return_sequences=True)),\n",
        "    Attention(),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LcMIM3IFY57T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ab382e-c6ca-44cb-814d-19efc14a595b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 300, 400)          51578000  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 296, 200)          400200    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 296, 800)         1923200   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " attention_1 (Attention)     (None, 800)               1096      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 4806      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53,907,302\n",
            "Trainable params: 53,907,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(128945, 400, input_length=300),\n",
        "    tf.keras.layers.Conv1D(200, 5, activation='relu'),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(400, return_sequences=True)),\n",
        "    Attention(),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NlapgLWTtQ1D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute class frequencies\n",
        "class_freqs = np.bincount(train_labels)\n",
        "total_samples = np.sum(class_freqs)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = {}\n",
        "for i, freq in enumerate(class_freqs):\n",
        "    class_weights[i] = total_samples / (len(class_freqs) * freq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "b7Gu2xnW0nGa",
        "outputId": "56c890d7-7f78-4411-9465-d456bdb78121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "19134/35715 [===============>..............] - ETA: 23:35 - loss: 0.8148 - accuracy: 0.6862"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-4c79542687d0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m h = model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpadded_train_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Most up to date architecture: April 15, 2023 -> Fa\n",
        "\n",
        "#Having the callbacks option on means that the model stops once you are like over-generalizing... \n",
        "#Let's leave the callback option on for now...\n",
        "\n",
        "\n",
        "h = model.fit(\n",
        "    padded_train_sequences, train_labels,\n",
        "    validation_data=(val_sequence, val_labels),\n",
        "    epochs=3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "safXM8N7r1Hh"
      },
      "outputs": [],
      "source": [
        "# Train the model with class weights\n",
        "#h = model.fit(\n",
        "    padded_train_sequences, train_labels,\n",
        "    validation_data=(val_sequence, val_labels),\n",
        "    epochs=2,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
        "   ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9vH4glT0pvr"
      },
      "source": [
        "#Evaluating The Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D4Kl54K0rww"
      },
      "outputs": [],
      "source": [
        "def show_history(h):\n",
        "    epochs_trained = len(h.history['loss'])\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n",
        "    plt.ylim([0., 1.])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n",
        "    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "show_history(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKJGeWYD0uKW"
      },
      "outputs": [],
      "source": [
        "#Running the model on the test sequence and test labels... \n",
        "\n",
        "eval = model.evaluate(test_sequence, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVtCPVVV0wMU"
      },
      "outputs": [],
      "source": [
        "#preds = model.predict_classes(test_sequence)\n",
        "preds=model.predict(test_sequence) \n",
        "classes_x=np.argmax(preds,axis=1)\n",
        "preds.shape, test_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi85BmDv0xwm"
      },
      "source": [
        "##Error Analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0bPxr94oPwj"
      },
      "outputs": [],
      "source": [
        "#Dumb luck metric \n",
        "\n",
        "counts = df['label'].value_counts()\n",
        "dumb_luck = max(counts) / sum(counts)\n",
        "\n",
        "dumb_luck\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtVkH2y70zda"
      },
      "outputs": [],
      "source": [
        "#Creating an inference dataframe \n",
        "\n",
        "inf_df=pd.DataFrame({\n",
        "    'data':testtext,\n",
        "    \"labels_predicted\": classes_x                    \n",
        "})\n",
        "inf_df[\"labels_predicted_marked\"]=inf_df['labels_predicted'].apply(lambda x: index_to_classes[x])\n",
        "inf_df[\"actual_labels\"]=testlabel\n",
        "\n",
        "#Creating the labels index datastructure...\n",
        "\n",
        "inf_df[\"actual_label_index\"]=inf_df['actual_labels'].apply(lambda x: classes_to_index[x])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frs6-UaJ05J0"
      },
      "outputs": [],
      "source": [
        "#Checking the inf_df shape...\n",
        "\n",
        "inf_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djekeBFK06Xu"
      },
      "outputs": [],
      "source": [
        "#Making the correct prediction \n",
        "\n",
        "inf_df['correct_pred']=0 # first assigning all to 0.\n",
        "inf_df.loc[(inf_df['labels_predicted']==inf_df['actual_label_index']),'correct_pred']=1 # labelling 1 if the prediction is right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc0hQQnc0829"
      },
      "outputs": [],
      "source": [
        "# magnitutde of error\n",
        "inf_df['error_magnitude']=abs(inf_df['labels_predicted']-inf_df['actual_label_index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKRJBQcf0-X5"
      },
      "outputs": [],
      "source": [
        "# Count the frequency of each class in the error_magnitude column\n",
        "counts = inf_df['error_magnitude'].value_counts()\n",
        "\n",
        "# Create a histogram with one bar for each class\n",
        "plt.bar(counts.index, counts.values)\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.suptitle('Histogram of Error Magnitude in Fear Classification')\n",
        "plt.xlabel('Magnitude')\n",
        "plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWaZuOGY1CFg"
      },
      "source": [
        "##Saving the model and evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR0pNn031Gez"
      },
      "outputs": [],
      "source": [
        "#So this works but we need to make sure that we install Keras as a dependency \n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "#model.save('/content/drive/MyDrive/Sean/Emoclass_Dec2022/Model_Fear_151K_Feb23,2022')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2OS1oKQ1KVx"
      },
      "outputs": [],
      "source": [
        "#Exporting the evaluation dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTNJKbfw1NDi"
      },
      "outputs": [],
      "source": [
        "#Exporting the dataset \n",
        "\n",
        "#path = \"/content/drive/MyDrive/Sean/Emoclass/Train_Fear_Error_Analysis_151K_March8,2023.csv\"\n",
        "inf_df.to_csv(path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN/ohV+EgmGDWZHjzd3zT76",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}